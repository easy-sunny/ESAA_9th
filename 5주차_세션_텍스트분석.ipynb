{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbDTRcUQrUS3"
   },
   "source": [
    "# 1. 아래의 데이터를 이용하여 공부한 내용을 바탕으로 문제 2개를 만들고 답하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PNjdc83Bqdpd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"book\", quiet=True)\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1708938633085,
     "user": {
      "displayName": "이수미",
      "userId": "11363034705926854575"
     },
     "user_tz": -540
    },
    "id": "K9xoXfKzrbKv",
    "outputId": "04efc370-fb7a-4a93-cfbb-0a5e4d54afd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일 목록 확인\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1708938633086,
     "user": {
      "displayName": "이수미",
      "userId": "11363034705926854575"
     },
     "user_tz": -540
    },
    "id": "jo9kzrrqqaNc",
    "outputId": "24a88018-f1f1-4a4a-a62c-6f3abf65ec10"
   },
   "outputs": [],
   "source": [
    "bible = nltk.corpus.gutenberg.raw( 'bible-kjv.txt')\n",
    "sample_bible=bible[:9250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNftR22-riz3"
   },
   "source": [
    "### (1) 문제 1 : 토큰화 후 stop words 적용 후, 해당 텍스트에서 제거된 단어 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mLLGn_Rxrkxk"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    #문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    #분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rqidjKSFrl-2"
   },
   "outputs": [],
   "source": [
    "bible_tokens = tokenize_text(sample_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 68\n"
     ]
    }
   ],
   "source": [
    "print(type(bible_tokens), len(bible_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 has 34 words.\n",
      "Sentence 2 has 21 words.\n",
      "Sentence 3 has 13 words.\n",
      "Sentence 4 has 15 words.\n",
      "Sentence 5 has 21 words.\n",
      "Sentence 6 has 15 words.\n",
      "Sentence 7 has 11 words.\n",
      "Sentence 8 has 27 words.\n",
      "Sentence 9 has 30 words.\n",
      "Sentence 10 has 8 words.\n",
      "Sentence 11 has 11 words.\n",
      "Sentence 12 has 30 words.\n",
      "Sentence 13 has 28 words.\n",
      "Sentence 14 has 42 words.\n",
      "Sentence 15 has 40 words.\n",
      "Sentence 16 has 12 words.\n",
      "Sentence 17 has 66 words.\n",
      "Sentence 18 has 31 words.\n",
      "Sentence 19 has 47 words.\n",
      "Sentence 20 has 12 words.\n",
      "Sentence 21 has 33 words.\n",
      "Sentence 22 has 41 words.\n",
      "Sentence 23 has 30 words.\n",
      "Sentence 24 has 12 words.\n",
      "Sentence 25 has 37 words.\n",
      "Sentence 26 has 39 words.\n",
      "Sentence 27 has 59 words.\n",
      "Sentence 28 has 26 words.\n",
      "Sentence 29 has 56 words.\n",
      "Sentence 30 has 50 words.\n",
      "Sentence 31 has 46 words.\n",
      "Sentence 32 has 20 words.\n",
      "Sentence 33 has 11 words.\n",
      "Sentence 34 has 17 words.\n",
      "Sentence 35 has 31 words.\n",
      "Sentence 36 has 29 words.\n",
      "Sentence 37 has 80 words.\n",
      "Sentence 38 has 20 words.\n",
      "Sentence 39 has 31 words.\n",
      "Sentence 40 has 23 words.\n",
      "Sentence 41 has 48 words.\n",
      "Sentence 42 has 26 words.\n",
      "Sentence 43 has 43 words.\n",
      "Sentence 44 has 23 words.\n",
      "Sentence 45 has 22 words.\n",
      "Sentence 46 has 7 words.\n",
      "Sentence 47 has 25 words.\n",
      "Sentence 48 has 55 words.\n",
      "Sentence 49 has 28 words.\n",
      "Sentence 50 has 50 words.\n",
      "Sentence 51 has 38 words.\n",
      "Sentence 52 has 60 words.\n",
      "Sentence 53 has 33 words.\n",
      "Sentence 54 has 26 words.\n",
      "Sentence 55 has 18 words.\n",
      "Sentence 56 has 20 words.\n",
      "Sentence 57 has 24 words.\n",
      "Sentence 58 has 60 words.\n",
      "Sentence 59 has 47 words.\n",
      "Sentence 60 has 59 words.\n",
      "Sentence 61 has 30 words.\n",
      "Sentence 62 has 42 words.\n",
      "Sentence 63 has 18 words.\n",
      "Sentence 64 has 28 words.\n",
      "Sentence 65 has 13 words.\n",
      "Sentence 66 has 17 words.\n",
      "Sentence 67 has 28 words.\n",
      "Sentence 68 has 2 words.\n",
      "Total words in the Bible sample: 2095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_counts_per_sentence = [len(sentence) for sentence in bible_tokens]\n",
    "\n",
    "# 문장마다 단어 수를 출력\n",
    "for idx, count in enumerate(word_counts_per_sentence):\n",
    "    print(f\"Sentence {idx+1} has {count} words.\")\n",
    "\n",
    "# 전체 문장의 단어 수를 출력\n",
    "total_words = sum(word_counts_per_sentence)\n",
    "print(f\"Total words in the Bible sample: {total_words}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "all_tokens=[]\n",
    "\n",
    "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
    "for sentence in bible_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "    for word in sentence:\n",
    "        # 소문자로 모두 변환\n",
    "        word = word.lower()\n",
    "        # 토큰화된 개별 단어가 스톱워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 has 23 words.\n",
      "Sentence 2 has 12 words.\n",
      "Sentence 3 has 7 words.\n",
      "Sentence 4 has 9 words.\n",
      "Sentence 5 has 12 words.\n",
      "Sentence 6 has 10 words.\n",
      "Sentence 7 has 5 words.\n",
      "Sentence 8 has 14 words.\n",
      "Sentence 9 has 12 words.\n",
      "Sentence 10 has 6 words.\n",
      "Sentence 11 has 5 words.\n",
      "Sentence 12 has 19 words.\n",
      "Sentence 13 has 17 words.\n",
      "Sentence 14 has 27 words.\n",
      "Sentence 15 has 24 words.\n",
      "Sentence 16 has 6 words.\n",
      "Sentence 17 has 32 words.\n",
      "Sentence 18 has 21 words.\n",
      "Sentence 19 has 23 words.\n",
      "Sentence 20 has 6 words.\n",
      "Sentence 21 has 22 words.\n",
      "Sentence 22 has 27 words.\n",
      "Sentence 23 has 19 words.\n",
      "Sentence 24 has 6 words.\n",
      "Sentence 25 has 22 words.\n",
      "Sentence 26 has 21 words.\n",
      "Sentence 27 has 31 words.\n",
      "Sentence 28 has 14 words.\n",
      "Sentence 29 has 31 words.\n",
      "Sentence 30 has 27 words.\n",
      "Sentence 31 has 25 words.\n",
      "Sentence 32 has 12 words.\n",
      "Sentence 33 has 5 words.\n",
      "Sentence 34 has 8 words.\n",
      "Sentence 35 has 14 words.\n",
      "Sentence 36 has 14 words.\n",
      "Sentence 37 has 35 words.\n",
      "Sentence 38 has 10 words.\n",
      "Sentence 39 has 18 words.\n",
      "Sentence 40 has 12 words.\n",
      "Sentence 41 has 25 words.\n",
      "Sentence 42 has 14 words.\n",
      "Sentence 43 has 21 words.\n",
      "Sentence 44 has 11 words.\n",
      "Sentence 45 has 11 words.\n",
      "Sentence 46 has 4 words.\n",
      "Sentence 47 has 12 words.\n",
      "Sentence 48 has 35 words.\n",
      "Sentence 49 has 13 words.\n",
      "Sentence 50 has 30 words.\n",
      "Sentence 51 has 18 words.\n",
      "Sentence 52 has 36 words.\n",
      "Sentence 53 has 17 words.\n",
      "Sentence 54 has 17 words.\n",
      "Sentence 55 has 8 words.\n",
      "Sentence 56 has 9 words.\n",
      "Sentence 57 has 17 words.\n",
      "Sentence 58 has 35 words.\n",
      "Sentence 59 has 32 words.\n",
      "Sentence 60 has 29 words.\n",
      "Sentence 61 has 15 words.\n",
      "Sentence 62 has 20 words.\n",
      "Sentence 63 has 13 words.\n",
      "Sentence 64 has 14 words.\n",
      "Sentence 65 has 9 words.\n",
      "Sentence 66 has 12 words.\n",
      "Sentence 67 has 13 words.\n",
      "Sentence 68 has 2 words.\n",
      "Total words in the Bible sample: 1155\n"
     ]
    }
   ],
   "source": [
    "word_counts_per_sentence = [len(sentence) for sentence in all_tokens]\n",
    "\n",
    "# 문장마다 단어 수를 출력\n",
    "for idx, count in enumerate(word_counts_per_sentence):\n",
    "    print(f\"Sentence {idx+1} has {count} words.\")\n",
    "\n",
    "# 전체 문장의 단어 수를 출력\n",
    "total_words = sum(word_counts_per_sentence)\n",
    "print(f\"Total words in the Bible sample: {total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 문장 및 단어 토큰화를 진행하고 텍스트 내 단어를 세는 것은 단순히 리스트의 개수가 아니라는 점을 다시 한번 확인. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', ':']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', ':']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6fES2kkrmP0"
   },
   "source": [
    "### (2) 문제 2: 각 단어 별 카운트해서, 중요 단어 뽑아보기 (단순 카운트로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "a9vmAhP1rnIi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common words and their counts are:\n",
      "[('the', 218), (',', 129), ('and', 117), ('of', 77), ('.', 63)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of word tokens to count each word\n",
    "all_words = [word for sentence in bible_tokens for word in sentence]\n",
    "\n",
    "# Count the occurrence of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Find the most common word(s)\n",
    "most_common_words = word_counts.most_common(5)  # You can specify the number of most common words you want\n",
    "\n",
    "print(\"The most common words and their counts are:\")\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c3PlmC3lrneg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common words (excluding punctuation and numbers) and their counts are:\n",
      "[('god', 53), ('earth', 27), ('said', 21), ('every', 21), ('day', 17)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # 단어가 알파벳 문자로만 이루어져 있는지 확인\n",
    "    return bool(re.match('^[a-zA-Z]+$', word))\n",
    "\n",
    "# 모든 단어를 추출하고 유효한 단어만 필터링\n",
    "all_words = [word for sentence in all_tokens for word in sentence if is_valid_word(word)]\n",
    "\n",
    "# 단어 등장 횟수 계산\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# 가장 많이 등장하는 단어 추출\n",
    "most_common_words = word_counts.most_common(5)  # 가장 많이 등장하는 상위 5개의 단어를 가져옴\n",
    "\n",
    "print(\"The most common words (excluding punctuation and numbers) and their counts are:\")\n",
    "print(most_common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">중요 단어를 뽑을 때 stop_words를 하기 전과 하고 나서의 정보 차이가 컸기 때문에 자연어 처리에 있어서 데이터 클렌징이 얼마나 중요한지 확인. stop_words의 경우 쉼표 등을 제거하는 것이 아니고 문법적 요소 단어를 제거한다는 점을 명확하게 확인함 "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
