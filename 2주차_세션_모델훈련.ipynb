{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCu72vDHGMHo"
   },
   "source": [
    "## **| 모델 훈련 연습 문제**\n",
    "___\n",
    "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
    "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3g-_Dq9GiuT"
   },
   "source": [
    "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "릿지가 기본이 되지만 쓰이는 특성이 몇 개뿐이라고 의심되면 라쏘나 엘라스티넷이 낫다. 이 모델들은 불필요한 특성의 가중치를 0으로 만들어주기 때문. 특성 수 가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로 라쏘보다는 엘라스티넷을 선호"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pDjW5XcHPOt"
   },
   "source": [
    "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과적합이 일어난 것으로 추정 -> 조기종료를 통해 해결할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM7JbsLoy7b7"
   },
   "source": [
    "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 오차와 검증 오차가 거의 비슷하고 둘 다 높은 것은 과소적합 때문 \n",
    "\n",
    "따라서 편향이 높은 것이기 때문에 모델의 복잡도를 높이기 위해 규제 하이퍼파라미터를 줄여야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8tARu-ZzOGx"
   },
   "source": [
    "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
    "\n",
    "과대 적합을 감소시키기 위해. 모델의 가중치를 제한하기 가장 일반적인 방법이기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 릿지 회귀 대신 라쏘 회귀\n",
    "\n",
    "쓰이는 특성이 몇 개뿐이라고 의심되는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라쏘 회귀 대신 엘라스틱넷\n",
    "\n",
    "특성 수가 훈련 샘플 수보다 많거나 특성 몇개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로 라쏘보다는 엘라스티넷을 선호\n",
    "\n",
    "-> 라쏘는 특성 수가 샘플 수보다 많으면 최대 n개의 특성을 선택. 또한 여러 특성이 강하게 연관되어 있으면 이들 중 임의의 특성 하나를 선택하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIZpOEYJVIAV"
   },
   "source": [
    "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 iris 데이터를 활용해 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8pXDQ_fU8Nz0"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 알고리즘 대신에 직접 스플릿하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "\n",
    "total_size = len(X_with_bias)\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "tolerance = 10  # 허용 가능한 연속적인 성능 하락 횟수\n",
    "count = 0  # 연속적인 성능 하락 횟수 초기화\n",
    "\n",
    "for epoch in range(1000):\n",
    "    softmax_reg.fit(X_train, y_train)\n",
    "    y_val_predict = softmax_reg.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_predict)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(softmax_reg)\n",
    "        count = 0  # 새로운 최고 성능이 발생했으므로 연속 성능 하락 횟수 초기화\n",
    "    else:\n",
    "        count += 1\n",
    "        if count == tolerance:\n",
    "            print(\"Early stopping at epoch\", epoch)\n",
    "            break  # 조기 종료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    softmax_reg.fit(X_train, y_train)\n",
    "    y_val_predict = softmax_reg.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_predict)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(softmax_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 소프트 맥스 함수를 직접 정의해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타깃은 클래스 인덱스(0, 1 그리고 2)이지만 소프트맥스 회귀 모델을 훈련시키기 위해 필요한 것은 타깃 클래스의 확률입니다. 각 샘플에서 확률이 1인 타깃 클래스를 제외한 다른 클래스의 확률은 0입니다(다른 말로하면 주어진 샘플에 대한 클래스 확률이 원-핫 벡터입니다). 클래스 인덱스를 원-핫 벡터로 바꾸는 간단한 함수를 작성하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_valid_one_hot = to_one_hot(y_valid)\n",
    "Y_test_one_hot = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_iterations만큼의 반복 실행\n",
    "\n",
    "\n",
    "각 반복에서는 다음과 같은 작업을 수행\n",
    "- 입력 데이터에 대한 가중치와의 내적을 통해 예측 점수(logits)를 계산합니다.\n",
    "- 소프트맥스 함수를 사용하여 예측 클래스의 확률 분포(Y_proba)를 계산합니다.\n",
    "- 손실 함수를 사용하여 그래디언트를 계산\n",
    "\n",
    "그래디언트는 손실 함수를 각 파라미터에 대해 미분한 값으로, 파라미터를 업데이트하는 데 사용\n",
    "계산된 그래디언트를 사용하여 파라미터 행렬 Theta를 업데이트합니다. 이때 학습률 eta를 사용하여 그래디언트의 크기를 조절합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 10000\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.05741137,  0.18649144, -2.34034035],\n",
       "       [ 0.69889081,  0.21518329, -2.11131934],\n",
       "       [ 1.89902633,  0.13194926, -1.20600382],\n",
       "       [-3.4079844 ,  0.17898683,  3.39152256],\n",
       "       [-0.32890627, -1.46337249,  1.56010421]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6192 0.14735336097167556\n",
      "6193 0.14734336354611458 조기 종료!\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 10000\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "best_loss = np.infty\n",
    "tolerance = 1e-5  # 허용 오차 설정\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    \n",
    "    loss = -1/m * np.sum(Y_train_one_hot * np.log(Y_proba + epsilon))\n",
    "    \n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients\n",
    "    \n",
    "    if loss < best_loss - tolerance:  # 최근 손실값과 이전 손실값의 차이가 허용 오차 이내로 감소하지 않으면\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"조기 종료!\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
